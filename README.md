# Spark para ETL em Databricks e Synapse Analytics

Este documento Ã© um guia completo sobre como utilizar Apache Spark para processos de ETL (Extract, Transform, Load), focando nas plataformas **Azure Databricks** e **Azure Synapse Analytics**. Ele cobre conceitos essenciais, tipos de arquivos, prÃ¡ticas recomendadas, funÃ§Ãµes Ãºteis e exemplos de cÃ³digo.

---

## âœ¨ VisÃ£o Geral do Spark para ETL

Apache Spark Ã© uma engine distribuÃ­da de processamento de dados em larga escala, ideal para workloads de ETL devido Ã  sua performance e escalabilidade.

**Principais BenefÃ­cios:**
- Processamento paralelo e distribuÃ­do.
- Suporte a diversos formatos de arquivo (Parquet, CSV, JSON, Delta, Avro, ORC).
- IntegraÃ§Ã£o nativa com Databricks e Synapse.
- APIs em Python, Scala, SQL, R.

### ðŸ” Comparativo entre Plataformas

| Recurso                     | Databricks                         | Synapse Spark                    |
|----------------------------|------------------------------------|----------------------------------|
| Tipo de cluster            | Permanente ou job cluster          | Pool dinÃ¢mico                    |
| Linguagens suportadas     | Python, SQL, Scala, R              | Python, SQL, Scala               |
| Suporte Delta Lake         | Nativo                             | CompatÃ­vel com configuraÃ§Ãµes     |
| IntegraÃ§Ã£o com Data Lake   | Alta                               | Alta                             |
| Performance tuning         | AvanÃ§ado (Photon, AQE)             | Limitado                         |

## ðŸ“š Tipos de Arquivos Suportados

| Tipo     | Leitura | Escrita | ObservaÃ§Ãµes                                            |
|----------|---------|---------|--------------------------------------------------------|
| CSV      | âœ…      | âœ…      | Formato texto simples, bom para interoperabilidade. Lento para grandes volumes. |
| JSON     | âœ…      | âœ…      | Suporta estruturas aninhadas. Pode ser pesado para leitura e parsing.          |
| Parquet  | âœ…      | âœ…      | Colunar, altamente eficiente em leitura e compressÃ£o. Recomendado para anÃ¡lise. |
| Delta    | âœ…      | âœ…      | ExtensÃ£o do Parquet com suporte a transaÃ§Ãµes ACID, versionamento e time travel. Ideal para pipelines. |
| Avro     | âœ…      | âœ…      | Compactado, schema embutido. Ã“timo para troca de dados entre sistemas.         |
| ORC      | âœ…      | âœ…      | Semelhante ao Parquet, mas mais usado no ecossistema Hadoop. Alta compressÃ£o.  |

> ðŸ’¡ **RecomendaÃ§Ã£o**: Prefira Parquet ou Delta para grandes volumes de dados em ambientes analÃ­ticos, especialmente se houver necessidade de consultas otimizadas.
---

## âš™ï¸ Iniciando o Spark (PySpark)

```python
from pyspark.sql import SparkSession

spark = (SparkSession.builder
    .appName("ETL Spark")
    .getOrCreate())
```

## ðŸ§± Conceitos Iniciais com RDD

Antes do uso de DataFrames, o Spark operava com RDDs (Resilient Distributed Datasets), uma estrutura de dados distribuÃ­da e tolerante a falhas.

```python
# Criando RDD a partir de lista
rdd = spark.sparkContext.parallelize([("cliente1", 25), ("cliente2", 30)])

# TransformaÃ§Ãµes
rdd_filtrado = rdd.filter(lambda x: x[1] > 26)

# AÃ§Ãµes
print(rdd_filtrado.collect())
```

Principais operaÃ§Ãµes:
- `map`, `flatMap`, `filter`
- `reduce`, `collect`, `count`
- `groupByKey`, `reduceByKey`, `join`

---

## ðŸš€ ETL com Spark: Etapas Fundamentais

### 1. ExtraÃ§Ã£o (Extract)
Obter dados de fontes diversas:
- Arquivos (CSV, JSON, Parquet, Delta, etc.)
- Bancos de dados (JDBC)
- APIs
- Armazenamento em nuvem (Azure Data Lake, Blob Storage)

### 2. TransformaÃ§Ã£o (Transform)
- Limpeza de dados (nulls, duplicatas)
- ConversÃ£o de tipos
- NormalizaÃ§Ã£o e agregados
- Joins e filtros

### 3. Carga (Load)
- Salvar dados transformados em:
  - Data Lakes (Parquet/Delta)
  - Data Warehouses (Synapse SQL Pools)
  - Outros sistemas analÃ­ticos

---

## ðŸ§ª 50 FunÃ§Ãµes Ãšteis para TransformaÃ§Ã£o com PySpark

### ðŸ”¡ ManipulaÃ§Ã£o de Strings
```python
upper(col("nome"))
lower(col("nome"))
trim(col("campo"))
substring("cpf", 1, 3)
concat_ws("-", "ano", "mes")
split(col("nome"), " ")
regexp_replace("tel", "[^0-9]", "")
initcap(col("nome"))
length(col("descricao"))
instr(col("descricao"), "abc")
```

### ðŸ§® ManipulaÃ§Ã£o NumÃ©rica
```python
round(col("valor"), 2)
abs(col("saldo"))
pow(col("base"), 2)
sqrt(col("valor"))
log(col("valor"))
exp(col("valor"))
bround(col("valor"), 1)
```

### ðŸ“† ManipulaÃ§Ã£o de Datas
```python
to_date("data", "yyyy-MM-dd")
to_timestamp("data_hora")
current_date()
current_timestamp()
datediff("data_fim", "data_ini")
months_between("fim", "inicio")
add_months("data", 2)
next_day("data", "Monday")
last_day("data")
```

### ðŸ”„ Condicionais e Nulos
```python
when(col("idade") > 18, "adulto").otherwise("menor")
coalesce("email", "email_backup")
isnull("campo")
isnotnull("campo")
```

### ðŸ§  FunÃ§Ãµes AnalÃ­ticas (Janela)
```python
from pyspark.sql.window import Window

windowSpec = Window.partitionBy("cliente").orderBy("data")

row_number().over(windowSpec)
rank().over(windowSpec)
lag("valor", 1).over(windowSpec)
lead("valor", 1).over(windowSpec)
sum("valor").over(windowSpec)
avg("valor").over(windowSpec)
```

### ðŸ”§ Outras funÃ§Ãµes Ãºteis
```python
col("campo")
lit("valor")
explode(col("lista"))
array_contains(col("categorias"), "x")
countDistinct("cliente_id")
sha2(col("id"), 256)
broadcast(df2)
approx_count_distinct("id")
```

---

## ðŸ” Extras

### Schema ExplÃ­cito
```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("nome", StringType(), True),
    StructField("idade", IntegerType(), True)
])

df = spark.read.schema(schema).csv("/mnt/raw/clientes.csv")
```

### Particionamento ao Salvar
```python
(df.write
    .partitionBy("ano", "mes")
    .mode("overwrite")
    .format("delta")
    .save("/mnt/gold/vendas"))
```

### Carga Incremental
```python
(df.filter(col("data") >= lit("2023-01-01"))
   .write
   .mode("append")
   .format("delta")
   .save("/mnt/gold/vendas"))
```

---

## ðŸ”— ReferÃªncias

- [DocumentaÃ§Ã£o oficial do Apache Spark](https://spark.apache.org/docs/latest/)
- [Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/)
- [Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/)
- [Delta Lake](https://delta.io/)

---

> Desenvolvido por Walter Gonzaga - [WGG Digital Solutions](https://www.linkedin.com/in/waltergonzaga/)
